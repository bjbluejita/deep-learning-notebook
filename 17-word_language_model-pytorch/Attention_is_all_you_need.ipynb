{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "源自：\n",
    "> http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "from torch.autograd import  Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context( context='talk' )\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder( nn.Module ):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this          and many other models.\n",
    "    '''\n",
    "    def __init__( self, encoder, decoder, src_embed, tgt_embed, generator ):\n",
    "        super( EncoderDecoder, self ).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward( self, src, tgt, src_mask, tgt_mask ):\n",
    "        '''Take in and process masked src and target sequences.'''\n",
    "        return self.decode( self.encode( src, src_mask ), src_mask, tgt, tgt_mask )\n",
    "\n",
    "    def encode( self, src, src_mask ):\n",
    "        return self.encoder( self.src_embed( src ), src_mask )\n",
    "    \n",
    "    def decode( self, memory, src_mask, tgt, tgt_mask ):\n",
    "        return self.decoder( self.tgt_embed( tgt ), memory, src_mask, tgt_mask )\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator( nn.Module ):\n",
    "    '''Define standard linear + softmax generation step.'''\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Generator, self ).__init__()\n",
    "        self.proj = nn.Linear( d_model, vocab )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return F.log_softmax( self.proj( x ), dim=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones( module, N ):\n",
    "    '''Produce N identical layers.'''\n",
    "    return nn.ModuleList( [ copy.deepcopy( module ) for _ in range( N ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder( nn.Module ):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Encoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, mask )\n",
    "        return self.norm( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__( self, features, eps=1e-6 ):\n",
    "        super( LayerNorm, self ).__init__()\n",
    "        self.a_2 = nn.Parameter( torch.ones( features ) )\n",
    "        self.b_2 = nn.parameter( torch.zeros( features ) )\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward( self, x ):\n",
    "        mean = x.mean( -1, keepdim=True )\n",
    "        std = x.std( -1, keepdim=True )\n",
    "        return self.a_2 * ( x - mean ) / ( std * self.eps ) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection( nn.Module ):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    '''\n",
    "    def __init__( self, size, dropout ):\n",
    "        super( SublayerConnection, self ).__init__()\n",
    "        self.norm = LayerNorm( size )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x, sublayer ):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout( sublayer( self.norm( x ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network.<br>\n",
    "![Transformer] (http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png \"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init( self, size, self_attn, feed_forward, dropout ):\n",
    "        super( EncoderLayer, self ).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout=dropout ), 2 )\n",
    "        self.size = size\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, mask ) )\n",
    "        return self.sublayer[1]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder( nn.Module ):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init( self, layer, N ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, memory, src_mask, tgt_mask )\n",
    "        return self.norm( x ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer( nn.Module ):\n",
    "    \"Decoder is made of self_attn, src_attn, and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, src_attn, feed_forward, dropout ):\n",
    "        super( DecoderLayer, self ).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout ), 3 )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, tgt_mask ) )\n",
    "        x = self.sublayer[1]( x, lambda x: self.src_attn( x, x, x, src_mask ) )\n",
    "        return self.sublayer[2]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3c91cf88f4eb>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-3c91cf88f4eb>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x148b66a0>"
     },
     "metadata": {},
     "execution_count": 11
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"314.881562pt\" version=\"1.1\" viewBox=\"0 0 331.245312 314.881562\" width=\"331.245312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 314.881562 \r\nL 331.245312 314.881562 \r\nL 331.245312 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 48.745313 282.5 \r\nL 320.545312 282.5 \r\nL 320.545312 10.7 \r\nL 48.745313 10.7 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p1f71a3128a)\">\r\n    <image height=\"272\" id=\"image4dca8fdb6f\" transform=\"scale(1 -1)translate(0 -272)\" width=\"272\" x=\"48.745313\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAARAAAAEQCAYAAAB4CisVAAAABHNCSVQICAgIfAhkiAAAA7xJREFUeJzt27Ftw0AUBUHbUBWKVYC7cP9VuApBzhUI1oLk8ciZ/ICLFj95n/ff2+MDIPga/QFgXgICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZJfRHwDG+7l+p3cuECATECATECATECATECATECATECATECATECATECATECATECAzpoODqcO4wgUCZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZNa4sFNbrmorFwiQCQiQCQiQCQiQCQiQCQiQCQiQCQiQCQiQCQiQCQiQCQiQGdPBBmYYxhUuECATECATECATECATECATECATECATECATECATECATECATECATECCzxoU3HHVVW7lAgExAgExAgExAgExAgExAgExAgExAgExAgExAgExAgExAgExAgMwal1Oyql2GCwTIBATIBATIBATIBATIBATIBATIBATIBATIBATIBATIBATIjOmYnmHcOC4QIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQILPGZTesaufjAgEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyYzpWYRh3Di4QIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQIBMQILPG5SWrWl5xgQCZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZNe5JWNWyBhcIkAkIkAkIkAkIkAkIkAkIkAkIkAkIkAkIkAkIkAkIkAkIkBnTTcgwjr1wgQCZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZgACZNe5AVrXMzgUCZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZMZ0CzGM44xcIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEBmjfvEqhb+zwUCZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZIdd41rVwvpcIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEA2xZjOMA72yQUCZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZAICZJuuca1q4VhcIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIECWx3SGcYALBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMgEBMj+ACQQEBDBXhDhAAAAAElFTkSuQmCC\" y=\"-10.5\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m5097801e83\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"55.540313\" xlink:href=\"#m5097801e83\" y=\"282.5\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(51.404688 304.977969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"123.490312\" xlink:href=\"#m5097801e83\" y=\"282.5\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(119.354687 304.977969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"191.440312\" xlink:href=\"#m5097801e83\" y=\"282.5\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(183.169062 304.977969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"259.390312\" xlink:href=\"#m5097801e83\" y=\"282.5\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(251.119062 304.977969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_5\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0c57704975\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"17.495\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(15.47125 22.433984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"51.47\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2.5 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(15.47125 56.408984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"85.445\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 5.0 -->\r\n      <g transform=\"translate(15.47125 90.383984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"119.42\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 7.5 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(15.47125 124.358984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"153.395\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10.0 -->\r\n      <g transform=\"translate(7.2 158.333984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"187.37\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 12.5 -->\r\n      <g transform=\"translate(7.2 192.308984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"221.345\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 15.0 -->\r\n      <g transform=\"translate(7.2 226.283984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"48.745313\" xlink:href=\"#m0c57704975\" y=\"255.32\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 17.5 -->\r\n      <g transform=\"translate(7.2 260.258984)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 48.745313 282.5 \r\nL 48.745313 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 320.545312 282.5 \r\nL 320.545312 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 48.745313 282.5 \r\nL 320.545312 282.5 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 48.745313 10.7 \r\nL 320.545312 10.7 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p1f71a3128a\">\r\n   <rect height=\"271.8\" width=\"271.8\" x=\"48.745313\" y=\"10.7\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAE7CAYAAABUlaNfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw1KSGhiWktTAsJSkBoalJDUwLCWpwf8Dm2VeIxBIu/wAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def subsequent_mask( size ):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = ( 1, size, size )\n",
    "    subsequent_mask = np.triu( np.ones( attn_shape ), k=1 ).astype( 'uint8' )\n",
    "    return torch.from_numpy( subsequent_mask ) == 0\n",
    "\n",
    "plt.figure( figsize=( 5, 5 ) )\n",
    "plt.imshow( subsequent_mask( 20 )[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "<br>\n",
    "![]Attent\n",
    "ion ()http://nl p.seas.harvard.edu/images/the-annotated-transformer_33_0.png, 'Attento\"\"' \"<\n",
    "><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention( query, key, value, mask=None, dropout=None ):\n",
    "    \"Compute 'Scaled Dot Product Attention\"\n",
    "    d_k = query.size( -1 )\n",
    "    scores = torch.matmul( query, key.transpose( -2, -1 )) \\\n",
    "             / math.sqrt( d_k )\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill( mask == 0.0, -1e9 )\n",
    "    p_attn = F.softmax( scores, dim=-1 )\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout( p_attn )\n",
    "    return torch.matmul( p_attn, value ), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHead:<br>\n",
    "\n",
    "<img  src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\" width=25% height=25% />\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, WKi∈Rdmodel×dk, WVi∈Rdmodel×dv and WO∈Rhdv×dmodel. In this work we employ h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeaderAttention( nn.Module ):\n",
    "    def __init__( self, h, d_model, dropout=0.1 ):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        supper( MultiHeaderAttention, self ).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones( nn.Linear( d_model, d_model ), 4 )\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, query, key, value, mask=None ):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze( 1 )\n",
    "        nbatches = query.size( 0 )\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [ l(x).view( nbatches, -1, self.h, self.d_k ).transpose( 1, 2 )\n",
    "              for l, x in zip( self.linears, ( query, key, value )) ]\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout )\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose( 1, 2 ).contingous() \\\n",
    "            .view( nbatches, -1, self.h * self.d_k )\n",
    "        return self.linears[-1]( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}