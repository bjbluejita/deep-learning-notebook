{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need\n",
    "源自：\n",
    "> http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/48731949\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/82312421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "from torch.autograd import  Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context( context='talk' )\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network.<br>\n",
    "<img src=\"https://pic1.zhimg.com/v2-1d9129c9c0d5367591bd093f79155e40_r.jpg\" height=\"600\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder( nn.Module ):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this          and many other models.\n",
    "    '''\n",
    "    def __init__( self, encoder, decoder, src_embed, tgt_embed, generator ):\n",
    "        super( EncoderDecoder, self ).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward( self, src, tgt, src_mask, tgt_mask ):\n",
    "        '''Take in and process masked src and target sequences.'''\n",
    "        return self.decode( self.encode( src, src_mask ), src_mask, tgt, tgt_mask )\n",
    "\n",
    "    def encode( self, src, src_mask ):\n",
    "        return self.encoder( self.src_embed( src ), src_mask )\n",
    "    \n",
    "    def decode( self, memory, src_mask, tgt, tgt_mask ):\n",
    "        return self.decoder( self.tgt_embed( tgt ), memory, src_mask, tgt_mask )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator( nn.Module ):\n",
    "    '''Define standard linear + softmax generation step.'''\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Generator, self ).__init__()\n",
    "        self.proj = nn.Linear( d_model, vocab )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return F.log_softmax( self.proj( x ), dim=-1 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先需要定义克隆函数，因为在多头注意力机制的实现中，用到多个结构相同的线性层。\n",
    "# 我们将使用clone函数将他们一同初始化在一个网络层列表对象中，之后的结构中也会用到该函数\n",
    "def clones( module, N ):\n",
    "    '''Produce N identical layers.'''\n",
    "    '''用于生成相同网络层的克隆函数，它的参数module表示要克隆的目标网络层，N代表需要克隆的数量'''\n",
    "    # 在函数中，我们通过for循环对module进行N次深度拷贝，使其每个module成为独立的层\n",
    "    # 然后将其放在nn.ModuleList类型的列表中存放，\n",
    "    return nn.ModuleList( [ copy.deepcopy( module ) for _ in range( N ) ] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>编码器(Encoder)</strong><br>\n",
    "编码器用于对输入进行指定的特征提取过程,也称为编码,由N个编码器层堆叠而成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder( nn.Module ):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__( self, layer, N ):\n",
    "        \"\"\"初始化函数的两个参数分别代表编码器层和编码器层的个数\"\"\"\n",
    "        super( Encoder, self ).__init__()\n",
    "        # 首先使用clones函数克隆N个编码器层放在self.layers中\n",
    "        self.layers = clones( layer, N )\n",
    "        # 再初始化一个规范化层，它将用在编码器的最后面\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        \"\"\"forward函数的输入和编码器层相同，x代表上一层的输出，mask代表掩码张量\"\"\"\n",
    "        # 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的X，\n",
    "        # 这个循环的过程，就相当于输出的x经过了N个编码器层的处理.\n",
    "        # 最后再通过规范化层的对象selfnorm进行处理，最后返回结果\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, mask )\n",
    "        return self.norm( x )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>编码层(EncoderLayer)</strong><br>\n",
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-0203e83066913b53ec6f5482be092aa1_1440w.webp\" height=\"400\" width=\"300\" />\n",
    "\n",
    "EncoderLayer: 每层都有两个子层组成。第一个子层实现了“多头”的 Self-attention，第二个子层则是一个简单的Position-wise的全连接前馈网络。<br>\n",
    "作为编码器的组成单元,每个编码器层完成一次对输入的特征提取过程,即编码过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, feed_forward, dropout ):\n",
    "        \"\"\"它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码\n",
    "        第二个self_attn，之后我们将传入多头自注意力子层实例化对象，并且是自注意力机制，\n",
    "        第三个是feed_froward，之后我们将传入前馈全连接层实例化对象，\n",
    "        最后一个是置0比率dropout\"\"\"\n",
    "        super( EncoderLayer, self ).__init__()\n",
    "\n",
    "        #首先将self_attn和feed_forward传入其中\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        #如图所示，编码器层中有两个子层连接结构，所以使用clones函数进行克隆\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout=dropout ), 2 )\n",
    "\n",
    "        #把size传入其中\n",
    "        self.size = size\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        \"\"\"\"forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask.\"\"\"\n",
    "        #里面就是按照结构图左侧的流程，首先通过第一个子层连接结构，其中包含多头自注意力子层\n",
    "        #然后通过第二个子层连接结构，其中包含前馈全连接子层。最后返回结果.\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, mask ) )\n",
    "        return self.sublayer[1]( x, self.feed_forward )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>规范化层(LayerNorm)</strong><br>\n",
    ". 规范化层的作用:<br>\n",
    "它是所有深层网络模型都需要的标渝网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__( self, features, eps=1e-6 ):\n",
    "        \"\"\"初始化函数有两个参数，一个是features，表示词嵌入的维度\n",
    "        另一个是eps它是一个足够小的数，在规范化公式的分母中出现防止分母为0.默认是1e-6，\"\"\"\n",
    "        super( LayerNorm, self ).__init__()\n",
    "\n",
    "        #根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，\n",
    "        #也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是9，这两个张量就是规范化层的参数\n",
    "        # 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，\n",
    "        #使其即能满足规范化要求，又能不改变针对目标的表征最后使用nn.parameter封装，代表他们是模型的参数。\n",
    "        self.a_2 = nn.Parameter( torch.ones( features ) )\n",
    "        self.b_2 = nn.Parameter( torch.zeros( features ) )\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward( self, x ):\n",
    "        \"\"\"输入参数x代表来自上一层的输出\"\"\"\n",
    "        # 在函数中，先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.\n",
    "        #接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果\n",
    "        #最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可\n",
    "        mean = x.mean( -1, keepdim=True )\n",
    "        std = x.std( -1, keepdim=True )\n",
    "        return self.a_2 * ( x - mean ) / ( std + self.eps ) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>子层连接结构(SublayerConnection)</strong><br>\n",
    "如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接(跳跃连接)，因此我们把这一部分结构整体叫做子层连接 (代表子层及其链接结构)，在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection( nn.Module ):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    '''\n",
    "    def __init__( self, size, dropout ):\n",
    "        \"\"\"它输入参数有两个，size以及dropout， size一般是都是词嵌入维度的大小,\n",
    "        dropout本身是对模型结构中的节点数进行随机抑制的比率，\n",
    "        又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机\"\"\"\n",
    "        super( SublayerConnection, self ).__init__()\n",
    "        # 实例化了规范化对象self.norm\n",
    "        self.norm = LayerNorm( size )\n",
    "        # 又使用nn中预定义的droupout实例化一个self.dropout对象\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x, sublayer ):\n",
    "        \"\"\"“前向逻辑函数中，接收上一个层或者子层的输入作为第一个参数,\n",
    "        将该子层连接中的子层函数作为第二个参数”\"\"\"\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        # 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作.\n",
    "        # 随机停止一些网络中神经元的作用，来防止过拟合，最后还有一个add操作，\n",
    "        #因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出\n",
    "        return x + self.dropout( sublayer( self.norm( x ) ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "Decoder也是由N=6个相同层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder( nn.Module ):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, memory, src_mask, tgt_mask )\n",
    "        return self.norm( x ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>解码器层(DecoderLayer)</strong><br>\n",
    "<ul>\n",
    "<li> 由N个解码器层堆叠而成 </li>\n",
    "<li> 每个解码器层由三个子层连接结构组成</li>\n",
    "<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>\n",
    "<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>\n",
    "</ul>\n",
    "解码器层作用：作为解码器的组成单元,每个解码器层根据给定的输入目标方向进行特征提取操作，即解码过程"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(DecoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer( nn.Module ):\n",
    "    \"Decoder is made of self_attn, src_attn, and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, src_attn, feed_forward, dropout ):\n",
    "        \"\"\"初始化函数的参数有5个，分别是size，代表词嵌入的维度大小，同时也代表解码器层的尺寸\n",
    "        第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V,\n",
    "        第三个是src_attn，多头注意力对象，这里Q!=K=V， \n",
    "        第四个是前馈全连接层对象，\n",
    "        最后就是dropout\"\"\"\n",
    "        super( DecoderLayer, self ).__init__()\n",
    "\n",
    "        #在初始化函数中，主要就是将这些输入传到类中\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        #按照结构图使用clones函数克隆三个子层连接对象\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout ), 3 )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        \"\"\"“forward函数中的参数有4个，分别是来自上一层的输入x,\n",
    "        来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量\"\"\"\n",
    "\n",
    "        #将memory表示成m方便之后使用\n",
    "        m = memory\n",
    "\n",
    "        #将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q K V都是x\n",
    "        #最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据\n",
    "        # 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，\n",
    "        # 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词时\n",
    "        # 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用，\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, tgt_mask ) )\n",
    "\n",
    "        #接着进入第二个子层，这个子层中常规的注意力机制，q是输入x;k，v是编码层输出memory,\n",
    "        # 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字筱而产生的注意力值\n",
    "        # 以此提升模型效果和训练速度，这样就完成了第二个子层的处理\n",
    "        x = self.sublayer[1]( x, lambda x: self.src_attn( x, m, m, src_mask ) )\n",
    "\n",
    "        #最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构\n",
    "        return self.sublayer[2]( x, self.feed_forward )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还修改了解码器中的Self-attetion子层以防止当前位置attend到后续位置。这种Masked的Attention是考虑到输出Embedding会偏移一个位置，确保了生成位置i的预测时，仅依赖于i的位置处已知输出，相当于把后面不该看的信心屏蔽掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def subsequent_mask( size ):\n",
    "    # \"Mask out subsequent positions.\"\n",
    "    '''生成向后遮掩的掩码张量，参数size是掩码张量最后两个维度的大小，它的最后两维形成一个方阵'''\n",
    "    #在函数中，首先定义掩码张量的形状\n",
    "    attn_shape = ( 1, size, size )\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵，最后为了节约空间\n",
    "    # #再使其中的数据类型变为无符号8位整形unit8\n",
    "    subsequent_mask = np.triu( np.ones( attn_shape ), k=1 ).astype( 'uint8' )\n",
    "\n",
    "    # 最后将numpy类型转化为torch中的tensor，内部做一个1 - 的操作\n",
    "    # 在这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减\n",
    "    # 如果是0，subsequent_mask中的该位置由0变成1\n",
    "    # 如果是1，subsequent_mask中的该位置由1变成9\n",
    "    return torch.from_numpy( subsequent_mask ) == 0\n",
    "\n",
    "plt.figure( figsize=( 5, 5 ) )\n",
    "plt.imshow( subsequent_mask( 7 )[0] )\n",
    "print( subsequent_mask( 7 ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出效果分析\n",
    "\n",
    "通过观察可视化方阵黄色是True的部分,这里代表被遮掩,紫色代表没有被遮掩的信息,横坐标代表目标词汇的位置,纵坐标代表可查看的位置,\n",
    "我们看到,在0的位置我们一看望过去都是黄色的,都被遮住了，1的位置一眼望过去还是黄色,说明第一次词还没有产生,从第二个位置看过去,就能看到位置1的词,其他位置看不到以此类推\n",
    "\n",
    "掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，.代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩,他可以说被替换,它的表现形式是一个张量\n",
    "在transformer中,掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attetion张量中的值计算有可能已知量未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用.所以，我们会进行遮掩.\n",
    "subsequent_mask输出是一个最后两维形成1方阵的下三角阵"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "它需要三个指定的输入Q(query),K(key),V(value),然后通过公式得到注意力的计算结果这个结果代表query在key和value作用下的表示而这个具体的计算规则有很多种,我这里只介绍我们用到的这一种:\n",
    "\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-c5dcddf20d8b2d7ce0130fac2071317d_720w.jpg\" />\n",
    "\n",
    "self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，对于要屏蔽的部分，mask之后的输出需要为负无穷，这样softmax之后输出才为0\n",
    "\n",
    "<font size=\"4\" color=\"white\">Q,K,V的比喻解释:</font><br>\n",
    "假如我们有一个问题: 结出一段文本，使用一些关键词对它进行描述!\n",
    "为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示,其中这些给出的提示就可以看作是key，\n",
    "而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的的答案信息，\n",
    "这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，\n",
    "因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多\n",
    "并且能够开始对我们query也就是这段文本，提取关键信息进行表示。 这就是注意力作用的过程， 通过这个过程\n",
    "我们最终脑子里的value发生了变化，\n",
    "根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法\n",
    "\n",
    "刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式,但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子\n",
    "使用一般注意力机制，是使用不同于给定文本的关键词表示它。而自注意力机制，\n",
    "需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它，相当于对文本自身的一次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention( query, key, value, mask=None, dropout=None ):\n",
    "    \"Compute 'Scaled Dot Product Attention\"\n",
    "    '''注意力机制的实现，输入分别是query，key，value，mask: 掩码张量\n",
    "    dropout是nn.Dropout层的实例化对象，默认为None\n",
    "    它的输出有两个,query的注意力表示以及注意力张量'''\n",
    "    #在函数中，首先取query的最后一维的大小，一般情况下就等同于我们的词嵌入维度，命名为d_k    \n",
    "    d_k = query.size( -1 )\n",
    "    #按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数\n",
    "    # 得到注意力得分张量scores\n",
    "    scores = torch.matmul( query, key.transpose( -2, -1 )) \\\n",
    "             / math.sqrt( d_k )\n",
    "    \n",
    "    #接着判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "        # 使用tensor的masked_fi11方法，将掩码张量和scores张量每个位置--比较，如果掩码张量\n",
    "        # 则对应的scores张量用-1e9这个值来替换，如下演示\n",
    "        scores = scores.masked_fill( mask == 0.0, -1e9 )\n",
    "\n",
    "    # 对scores的最后一维进行softmax操作，使用F.softmax方法，第一个参数是softmax对象，第二个\n",
    "    #这样获得最终的注意力张量\n",
    "    p_attn = F.softmax( scores, dim=-1 )\n",
    "\n",
    "    # 之后判断是否使用dropout进行随机置9\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout( p_attn )\n",
    "    \n",
    "    #最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量\n",
    "    return torch.matmul( p_attn, value ), p_attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHead:<br>\n",
    "\n",
    "<img  src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\" width=25% height=25% />\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, WKi∈Rdmodel×dk, WVi∈Rdmodel×dv and WO∈Rhdv×dmodel. In this work we employ h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "\n",
    "“多头”机制能让模型考虑到不同位置的Attention，另外“多头”Attention可以在不同的子空间表示不一样的关联关系，使用单个Head的Attention一般达不到这种效果。\n",
    "\n",
    "+ <font size='5'>多头注意力机制的作用:</font><br>\n",
    "<font size='3' color=\"white\" >从多头注意力的缩构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是,我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量.这就是所谓的多头，将每个头的获得的输入送到注意力机制中,就形成多头注意力机制<br>\n",
    "1. 这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention( nn.Module ):\n",
    "    def __init__( self, h, d_model, dropout=0.1 ):\n",
    "        '''在类的初始化时，会传入三个参数，h(head)代表头数，d_model代表词嵌入的维度\n",
    "        dropout代表进行dropout操作时置0比率，默认是0.1.'''\n",
    "        super( MultiHeadedAttention, self ).__init__()\n",
    "\n",
    "        # 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除\n",
    "        # 这是因为我们之后要给每个头分配等量的词特征.也就是d_model/head个\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        #得到每个头获得的分割词向量维度d_k\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.h = h\n",
    "\n",
    "        # 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是d_model x d_model\n",
    "        #为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个\n",
    "        self.linears = clones( nn.Linear( d_model, d_model ), 4 )\n",
    "\n",
    "        # self.attn为None.它代表最后得到的注意力张量，现在还没有结果所以为None.\n",
    "        self.attn = None\n",
    "\n",
    "        # 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, query, key, value, mask=None ):\n",
    "        \"前向逻辑函数，它的输入参数有四个，前三个就是注意力机制需要的Q，K，V,最后一个是注意力机制中可能需要的mask掩码张量，默认是None.\"\n",
    "\n",
    "        #如果存在掩码张量mask\n",
    "        if mask is not None:\n",
    "            # 使用unsqueeze拓展维度，代表多头中的第n头\n",
    "            mask = mask.unsqueeze( 1 )\n",
    "        \n",
    "        # 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本\n",
    "        nbatches = query.size( 0 )\n",
    "\n",
    "        # 就进入多头处理环节:\n",
    "        #首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中\n",
    "        #做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加1个维度h，代表头\n",
    "        #这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度\n",
    "        #计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，\n",
    "        #为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，\n",
    "        # 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头输入\n",
    "        query, key, value = \\\n",
    "            [ l(x).view( nbatches, -1, self.h, self.d_k ).transpose( 1, 2 )\n",
    "              for l, x in zip( self.linears, ( query, key, value )) ]\n",
    "        \n",
    "        #得到每个头的输入后，接下来就是将他们传入到attention中，\n",
    "        # 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中\n",
    "        x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout )\n",
    "\n",
    "        #通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的\n",
    "        #因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiquous方法\n",
    "        #这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，\n",
    "        #所以，下一步就是使用view重塑形状，变成和输入形状相同.\n",
    "        x = x.transpose( 1, 2 ).contiguous() \\\n",
    "            .view( nbatches, -1, self.h * self.d_k )\n",
    "        \n",
    "        #最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.\n",
    "        return self.linears[-1]( x )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    ". 学习了什么是多头注意力机制:<br>\n",
    "<p>每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后-维的词嵌入向量.这就是所谓的多头.将每个头的获得的输入送到注意力机制中,就形成类多头注意力机制</p>\n",
    "<p>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>前馈全连接层:</strong><br>\n",
    "前馈全连接层就是具有两层线性层的全连接网络,考虑注意力机制可能对复杂过程的拟合程度不够,通过增加两层网络来增强模型的能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward( nn.Module ):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__( self, d_model, d_ff, dropout=0.1 ):\n",
    "        \"\"\"初始化函数有三个输入参数分别是d_model，d_ff,和dropout=0.1，\n",
    "        第一个是线性层的输入维因为我们希望输入通过前馈全连接层后输入和输出的维度不变，\n",
    "        第二个参数d_ff就是第一个线性层输出的维度，和第二个线性层输入的维度\n",
    "        最后一个是dropout置0比率，\"\"\"\n",
    "        super( PositionwiseFeedForward, self ).__init__()\n",
    "        self.w_1 = nn.Linear( d_model, d_ff )\n",
    "        self.w_2 = nn.Linear( d_ff, d_model )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return self.w_2( self.dropout( F.relu( self.w_1( x ) )) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emedding 和 softmax\n",
    "\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-ca9c861576e2aac1ee7211d4e0bc6281_720w.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings( nn.Module ):\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Embeddings, self ).__init__()\n",
    "        self.lut = nn.Embedding( vocab, d_model )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward( self, x ):\n",
    "        #在forward函数中将输入x传入到Embedding的实例化对象中,\n",
    "        # 然后乘以一个根号下d_model进行缩放控制数值大小*它的输出是文本嵌入后的结果\n",
    "        return self.lut( x ) * math.sqrt( self.d_model ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding( nn.Module ):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__( self, d_model, dropout, max_len=5000 ):\n",
    "        #d_model:词嵌入维度\n",
    "        #dropout:置0比率\n",
    "        #max_len:每个句子重大长度\n",
    "        super( PositionalEncoding, self ).__init__()\n",
    "\n",
    "        #实例化nn中预定义的Dropout层，并将dropout传入其中，获得self.dropout\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        #初始化一个位置矩阵，矩阵大小max_len X d_model\n",
    "        pe = torch.zeros( max_len, d_model )\n",
    "\n",
    "        #初始化一个绝对位置矩阵，这里，词汇的绝对位置就是用它的索引表示\n",
    "        #所以首先用range获得一个连续自然数向量，然后再用unsqueeze拓展向量维度\n",
    "        position = torch.arange( 0, max_len ).unsqueeze( 1 ).float()\n",
    "\n",
    "        #绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中\n",
    "        #最简单思路就是先将max_len X 1的绝对位置矩阵， 变换成ax len x d_model形状，然后覆盖原来的初始位置编码矩阵即可\n",
    "        # 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外\n",
    "        #还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛， 这样我们就可以开始初始\n",
    "        # 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现，  我们这里并没有按照预计的一样初始化一个1xd_model的矩阵\n",
    "        # 而是有了一个跳跃，只初始化了一半即1xd_mode1/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵\n",
    "        #我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初\n",
    "        # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵，\n",
    "        div_term = torch.exp( torch.arange( 0., d_model, 2) * -(math.log(10000.0) / d_model ) )\n",
    "        pe[ :, 0::2 ] = torch.sin( position * div_term )\n",
    "        pe[ :, 1::2 ] = torch.cos( position * div_term )\n",
    "        # 这样我们就得到了位置编码矩阵pe，pe现在还只是一个二维矩阵，要想和embedding的输出(一个\n",
    "        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度\n",
    "        pe = pe.unsqueeze( 0 )\n",
    "        # 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢\n",
    "        # 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤更新的增益对象\n",
    "        #注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载\n",
    "        self.register_buffer( 'pe', pe )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        # forward函数的参数是x，表示文本序列的词嵌入美示\n",
    "        # 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切到与输入的x的第二维相同即x.size(1).\n",
    "        # 因为我们默认max len为5000一般来进实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配。\n",
    "        # 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.\n",
    "        # 在原有的信息上叠加了位置信息,保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化\n",
    "        # 正弦波和余弦波的值域范围都是1到-1,这又很好的控制了嵌入数值的大小有助于梯度的快速计算\n",
    "        x = x + Variable( self.pe[ :, :x.size(1) ], requires_grad=False )\n",
    "        return self.dropout( x )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制词汇向量中特征的分布曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=( 15, 5) )\n",
    "pe = PositionalEncoding( 20, 0.01 )\n",
    "#y = pe.forward( Variable( torch.zeros( 1, 100, 20 ) ))\n",
    "y = pe.forward( Variable( torch.rand( 1, 100, 20 ) ))\n",
    "print( y.shape )\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出效果分析:\n",
    ". 每条颜色的曲线代表某一个词汇中的特征在不同位置的含义 \n",
    ". 保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化 \n",
    ". 正弦波和余弦波的值域范围都是1到1这又很好的控制了嵌入数值的大小,有助于梯度的快速计算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we define a function that takes in hyperparameters and produces a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention( h, d_model )\n",
    "    ff = PositionwiseFeedForward( d_model, d_ff, dropout )\n",
    "    position = PositionalEncoding( d_model, dropout )\n",
    "\n",
    "    model = EncoderDecoder( \n",
    "        Encoder( EncoderLayer( d_model, c(attn), c(ff), dropout ), N ),\n",
    "        Decoder( DecoderLayer( d_model, c(attn), c(attn), c(ff), dropout ), N ),\n",
    "        nn.Sequential( Embeddings( d_model, src_vocab ), c(position) ),\n",
    "        nn.Sequential( Embeddings( d_model, tgt_vocab ), c(position) ),\n",
    "        Generator( d_model, tgt_vocab )\n",
    "    )\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform( p )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "# print( tmp_model )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch( ):\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__( self, src, trg=None, pad=0 ):\n",
    "        self.src = src\n",
    "        self.src_mask = ( src != pad ).unsqueeze( -2 )\n",
    "        if trg is not None:\n",
    "            self.trg = trg[ :, :-1 ]\n",
    "            self.trg_y = trg[ :, 1: ]\n",
    "            self.trg_mask = self.make_std_mask( self.trg, pad  )\n",
    "            self.ntokens = ( self.trg_y != pad ).data.sum()\n",
    "\n",
    "    @staticmethod       \n",
    "    def make_std_mask( tgt, pad ):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = ( tgt != pad ).unsqueeze( -2 )\n",
    "        tgt_mask = tgt_mask & Variable( \n",
    "            subsequent_mask( tgt.size( -1 ) ).type_as( tgt_mask.data )\n",
    "        )\n",
    "        return tgt_mask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch( data_iter, model, loss_compute ):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "\n",
    "    for i, batch in enumerate( data_iter ):\n",
    "        out = model.forward( batch.src, batch.trg, \\\n",
    "                             batch.src_mask, batch.trg_mask )\n",
    "        loss = loss_compute( out, batch.trg_y, batch.ntokens )\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print( 'Epoch Step: %4d Loss:%5.4f Tokens per sec  %4.2f' % ( i, loss / batch.ntokens, tokens / elapsed ))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn( new, count, sofar ):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max( max_src_in_batch, len( new.src ) )\n",
    "    max_tgt_in_batch = max( max_tgt_in_batch, len( new.trg ) + 2 )\n",
    "    src_elemnets = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "\n",
    "    return max( src_elemnets, tgt_elements )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We used the Adam optimizer (cite) with β1=0.9, β2=0.98 and ϵ=10−9.\n",
    "\n",
    ">> Note: This part is very important. Need to train with this setup of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt():\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__( self, model_size, factor, warmup, optimizer ):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step( self ):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[ 'lr' ] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate( self, step=None ):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "                ( self.model_size ** ( -0.5 ) * \n",
    "                   min( step ** ( -0.5 ), step * self.warmup ** ( -1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt( model ):\n",
    "    return NoamOpt( model.src_embed[0].d_model, 2, 4000,\n",
    "                  torch.optim.Adam( model.parameters(), lr=0, betas=( 0.9, 0.98 ), eps=1e-9 ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [ NoamOpt( 512, 1, 4000, None ),\n",
    "         NoamOpt( 1024, 1, 8000, None ),\n",
    "         NoamOpt( 256, 1, 4000, None )]\n",
    "plt.plot( np.arange( 1, 20000 ), [ [ opt.rate( i ) for opt in opts ] for i in range( 1, 20000 ) ])\n",
    "plt.legend( [ '512:4000', '1024:8000', '256:4000'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Label Smoothing\n",
    "  mmnnrnrenrennren令人迷惑，但确实能改善accuracy and BLEU 成绩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing( nn.Module ):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__( self, size, padding_idx, smoothing=0.0 ):\n",
    "        super( LabelSmoothing, self ).__init__()\n",
    "        self.criterion = nn.KLDivLoss( size_average=False )\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward( self, x, target ):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_( self.smoothing / ( self.size - 2 ) )\n",
    "        true_dist.scatter_( 1, target.data.unsqueeze(1), self.confidence )\n",
    "        true_dist[ :, self.padding_idx ] = 0\n",
    "        mask = torch.nonzero( target.data == self.padding_idx )\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill( 0, mask.squeeze(), 0.0 )\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion( x, Variable( true_dist, requires_grad=False ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing( 5, 0, 0.4 )\n",
    "predict = torch.FloatTensor( [[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]] )\n",
    "v = crit( Variable( predict.log() ), Variable( torch.LongTensor( [ 2, 1, 0 ] ) ) )\n",
    "plt.imshow( crit.true_dist )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当model对选择给出非常有信心时， sjissjsjisjiisj实际上会开始惩罚model（防止过拟？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = LabelSmoothing( 5, 0, 0.1 )\n",
    "def loss( x ):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor( [[ 0, x/d, 1/d, 1/d, 1/d ]] )\n",
    "    return crit( Variable( predict.log() ), Variable( torch.LongTensor( [1] ) ) ).item()\n",
    "\n",
    "plt.plot( np.arange(1, 100), [loss(x) for x in range(1, 100)] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "def data_gen( V, batch, nbatches ):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range( nbatches ):\n",
    "        data = torch.from_numpy( np.random.randint( 1, V, size=( batch, 10 ) ) )\n",
    "        data[ :, 0 ] = 1\n",
    "        src = Variable( data, requires_grad=False ).long()\n",
    "        tgt = Variable( data, requires_grad=False ).long()\n",
    "        yield Batch( src, tgt, 0 )\n",
    "\n",
    "# Loss Computation\n",
    "class SimpleLossCompute():\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__( self, generator, criterion, opt=None ):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__( self, x, y, norm ):\n",
    "        x = self.generator( x )\n",
    "        loss = self.criterion( x.contiguous().view( -1, x.size(-1) ),\n",
    "                               y.contiguous().view( -1 ) ) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Decoding\n",
    "V = 11\n",
    "criterion = LabelSmoothing( size=V, padding_idx=0, smoothing=0.0 )\n",
    "model = make_model( V, V, N=2 )\n",
    "model_opt = NoamOpt( model.src_embed[0].d_model, 1, 400,\n",
    "                     torch.optim.Adam( model.parameters(), lr=1, betas=( 0.9, 0.98 ), eps=1e-9 ) )\n",
    "\n",
    "for epoch in range( 10 ):\n",
    "    model.train()\n",
    "    run_epoch( data_gen( V, 30, 20 ), model, \n",
    "                SimpleLossCompute( model.generator, criterion, model_opt ))\n",
    "    model.eval()\n",
    "    print( run_epoch(data_gen( V, 30, 5 ), model, \n",
    "                    SimpleLossCompute( model.generator, criterion, None ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,9,4,5,6,7,4,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "    BLANK_WORD = \"<blank>\"\n",
    "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "    MAX_LEN = 100\n",
    "    train, val, test = datasets.IWSLT.splits( exts=( '.de', '.en' ), fields=( SRC, TGT ),\n",
    "                                               filter_pred=lambda x: len( vars(x)[ 'src']) <= MAX_LEN and\n",
    "                                                  len( vars(x)[ 'trg' ]) <= MAX_LEN )\n",
    "    MIN_FREQ = 2\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator( data.Iterator ):\n",
    "    def create_batches( self ):\n",
    "        if self.train:\n",
    "            def pool( d, random_shuffler ):\n",
    "                for p in data.batch( d, self.batch_size * 100 ):\n",
    "                    p_batch = data.batch( sorted( p, key=self.sort_key ),\n",
    "                                           self.batch_size, self.batch_size_fn )\n",
    "                    for b in random_shuffler( list( p_batch ) ):\n",
    "                        yield b\n",
    "            self.batches = pool( self.data(), self.random_shuffler )\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch( self.data(), self.batch_size, self.batch_size_fn ):\n",
    "                self.batches.append( sorted( b, key=self.sort_key ) )\n",
    "\n",
    "def rebatch( pad_idx, batch ):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose( 0, 1 ), batch.trg.transpose( 0, 1 )\n",
    "    return Batch( src, trg, pad_idx )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后为了快速训练，我们使用了多块     GPU。这段代码将实现多 GPU 的词生成，但它并不是针对 Transformer 的具体方法，所以这里并不会具体讨论。多 GPU 训练的基本思想即在训练过程中将词生成分割为语块（chunks），并传入不同的 GPU 实现并行处理，我们可以使用 PyTorch 并行基元实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__( self, generator, criterion, devices, opt=None, chunk_size=5 ):\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate( criterion, devices=devices )\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__( self, out, targets, normalize ):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate( self.generator, devices=devices )\n",
    "        out_scatter = nn.parallel.scatter( out, target_gpus=self.devices )\n",
    "        out_grad = [ [] for _ in out_scatter ]\n",
    "        targets = nn.parallel.scatter( targets, target_gpus=self.devices )\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range( 0, out_scatter[0].size(1), chunk_size ):\n",
    "            # Predict distributions\n",
    "            out_column = [ [ Variable( o[ :, i:i+chunk_size ].data, requires_grad=self.opt is not None )]\n",
    "                           for o in out_scatter ]\n",
    "            gen = nn.parallel.parallel_apply( generator, out_column )\n",
    "\n",
    "            # Compute loss\n",
    "            y = [ ( g.contigous().view( -1, g.size( -1 )), \n",
    "                   t[ :, i:i+chunk_size ].contigous().view( -1 ))\n",
    "                     for g, t in zip( gen, targets ) ]\n",
    "            loss = nn.parallel.parallel_apply( self.criterion, y )\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather( loss, target_device=self.devices[0] )\n",
    "            l = l.sum()[0] / normalize\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate( loss ):\n",
    "                    out_grad[ j ].append( out_column[ j ][ 0 ].grad.data.clone() ) \n",
    "\n",
    "        # Backprop loss to output of transformer\n",
    "        if self.opt is not None:\n",
    "            out_grad = [ Variable( torch.cat( og, dim=1 )) for og in out_grad ]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather( out_grad,\n",
    "                                     target_device=self.devices[0] )\n",
    "            o1.backward( gradient=o2 )\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "\n",
    "        return total * normalize                                     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用前面定义的函数创建了模型、度量标准、优化器、数据迭代器和并行化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [0]\n",
    "if True:\n",
    "    pad_idx = TGT.vocab.stoi[ '<blank>' ]\n",
    "    model = make_model( len(SRC.vocab ), len(TGT.vocab ), N=6 )\n",
    "    model.cuda()\n",
    "    criterion = LabelSmoothing( size=len( TGT.vocab ), padding_idx=pad_idx, smoothing=0.1 )\n",
    "    criterion.cuda()\n",
    "    BATCH_SIZE = 12000\n",
    "    train_iter = MyIterator( train, batch_size=BATCH_SIZE, device=0, repeat=False, \n",
    "                             sort_key=lambda x: ( len( x.src ), len( x.trg ) ), \n",
    "                             batch_size_fn=batch_size_fn, train=True )\n",
    "    valid_iter = MyIterator( val, batch_size=BATCH_SIZE, device=0, repeat=False,\n",
    "                              sort_key=lambda x: ( len( x.src ), len( x.trg ) ),\n",
    "                              batch_size_fn=batch_size_fn, train=False )\n",
    "    model_par = nn.DataParallel( model, device_ids=devices )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model_opt = NoamOpt( model.src_embed[0].d_model, 1, 2000,\n",
    "                         torch.optim.Adam( model.parameters(), lr=0, betas=( 0.9, 0.98 ), eps=1e-9 ) )\n",
    "\n",
    "    for epoch in range( 10 ):\n",
    "        model_par.train()\n",
    "        run_epoch( ( rebatch( pad_idx, b ) for b in train_iter ),\n",
    "                    model_par,\n",
    "                    MultiGPULossCompute( model.generator, criterion,\n",
    "                                         devices=devices, opt=model_opt ))\n",
    "        model_par.eval()\n",
    "        loss = run_epoch( ( rebatch( pad_idx, b ) for b in valid_iter ),\n",
    "                           model_par, \n",
    "                           MultiGPULossCompute( model.generator, criterion,\n",
    "                                                devices=devices, opt=None ) )\n",
    "        print( loss )\n",
    "else:\n",
    "    model = torch.load( 'iwslt.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, SRC, TGT = torch.load(\"E:/ML_data/attention/en-de-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sent = \"▁The ▁log ▁file ▁can ▁be ▁sent ▁secret ly ▁with ▁email ▁or ▁FTP ▁to ▁a ▁specified ▁receiver\".split()\n",
    "src = torch.LongTensor([[SRC.stoi[w] for w in sent]])\n",
    "src = Variable( src )\n",
    "src_mask = ( src != SRC.stoi[ '<blank>' ] ).unsqueeze( -2 )\n",
    "out = greedy_decode( model, src, src_mask, \n",
    "                     max_len=60, start_symbol=TGT.stoi[ '<s>' ] )\n",
    "\n",
    "print( 'Translation:', end='\\t' )\n",
    "trans = '<s>'\n",
    "for i in range( 1, out.size( 1 ) ):\n",
    "    sym =TGT.itos[ out[ 0, i ] ]\n",
    "    if sym == '</s>':\n",
    "        break\n",
    "    trans += sym + ' '\n",
    "print( trans )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sent = trans.split()\n",
    "def draw( data, x, y, ax ):\n",
    "    seaborn.heatmap( data, \n",
    "                     xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
    "                     cbar=False, ax=ax )\n",
    "\n",
    "for layer in range( 1, 6, 2 ):\n",
    "    fig, axs = plt.subplots( 1, 4, figsize=( 20, 10 ) )\n",
    "    print( 'Encoder layer: ', layer+1 )\n",
    "    for h in range( 4 ):\n",
    "        draw( model.encoder.layers[ layer ].self_attn.attn[ 0, h ].data,\n",
    "              sent, sent if h==0 else [], ax=axs[h] )\n",
    "    plt.show()\n",
    "\n",
    "for layer in range( 1, 6, 2 ):\n",
    "    fig, axs = plt.subplots( 1,4, figsize=( 20, 10 ) )\n",
    "    print( 'Decoder Self layer :', layer+1 )\n",
    "    for h in range( 4 ):\n",
    "        draw( model.decoder.layers[ layer ].self_attn.attn[ 0, h ].data[ :len( tgt_sent ), :len( tgt_sent )],\n",
    "              tgt_sent, tgt_sent if h==0 else [], ax=axs[h] )\n",
    "    plt.show()\n",
    "\n",
    "    print( 'Decoder Src Layer : ', layer+1 )\n",
    "    fig, axs = plt.subplots( 1, 4, figsize=( 20, 10 ) )\n",
    "    for h in range( 4 ):\n",
    "        draw( model.decoder.layers[ layer ].self_attn.attn[ 0, h ].data[ :len( tgt_sent ), :len( sent ) ],\n",
    "              sent, tgt_sent if h ==0 else [], ax=axs[h] )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
