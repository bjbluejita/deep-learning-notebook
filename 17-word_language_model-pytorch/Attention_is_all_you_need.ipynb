{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "from torch.autograd import  Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context( context='talk' )\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder( nn.Module ):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this          and many other models.\n",
    "    '''\n",
    "    def __init__( self, encoder, decoder, src_embed, tgt_embed, generator ):\n",
    "        super( EncoderDecoder, self ).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward( self, src, tgt, src_mask, tgt_mask ):\n",
    "        '''Take in and process masked src and target sequences.'''\n",
    "        return self.decode( self.encode( src, src_mask ), src_mask, tgt, tgt_mask )\n",
    "\n",
    "    def encode( self, src, src_mask ):\n",
    "        return self.encoder( self.src_embed( src ), src_mask )\n",
    "    \n",
    "    def decode( self, memory, src_mask, tgt, tgt_mask ):\n",
    "        return self.decoder( self.tgt_embed( tgt ), memory, src_mask, tgt_mask )\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator( nn.Module ):\n",
    "    '''Define standard linear + softmax generation step.'''\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Generator, self ).__init__()\n",
    "        self.proj = nn.Linear( d_model, vocab )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return F.log_softmax( self.proj( x ), dim=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% [markdown]\n",
    "\n",
    "# Encoder\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones( module, N ):\n",
    "    '''Produce N identical layers.'''\n",
    "    return nn.ModuleList( [ copy.deepcopy( module ) for _ in range( N ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder( nn.Module ):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Encoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, mask )\n",
    "        return self.norm( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% [markdown]\n",
    "\n",
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__( self, features, eps=1e-6 ):\n",
    "        super( LayerNorm, self ).__init__()\n",
    "        self.a_2 = nn.Parameter( torch.ones( features ) )\n",
    "        self.b_2 = nn.parameter( torch.zeros( features ) )\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward( self, x ):\n",
    "        mean = x.mean( -1, keepdim=True )\n",
    "        std = x.std( -1, keepdim=True )\n",
    "        return self.a_2 * ( x - mean ) / ( std * self.eps ) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection( nn.Module ):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    '''\n",
    "    def __init__( self, size, dropout ):\n",
    "        super( SublayerConnection, self ).__init__()\n",
    "        self.norm = LayerNorm( size )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x, sublayer ):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout( sublayer( self.norm( x ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% [markdown]\n",
    ".The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init( self, size, self_attn, feed_forward, dropout ):\n",
    "        super( EncoderLayer, self ).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout=dropout ), 2 )\n",
    "        self.size = size\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, mask ) )\n",
    "        return self.sublayer[1]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder( nn.Module ):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init( self, layer, N ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, memory, src_mask, tgt_mask )\n",
    "        return self.norm( x ) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}