{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention is All You Need\n",
    "源自：\n",
    "> http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/48731949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "from torch.autograd import  Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context( context='talk' )\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network.<br>\n",
    "![Transformer] (http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png \"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder( nn.Module ):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this          and many other models.\n",
    "    '''\n",
    "    def __init__( self, encoder, decoder, src_embed, tgt_embed, generator ):\n",
    "        super( EncoderDecoder, self ).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward( self, src, tgt, src_mask, tgt_mask ):\n",
    "        '''Take in and process masked src and target sequences.'''\n",
    "        return self.decode( self.encode( src, src_mask ), src_mask, tgt, tgt_mask )\n",
    "\n",
    "    def encode( self, src, src_mask ):\n",
    "        return self.encoder( self.src_embed( src ), src_mask )\n",
    "    \n",
    "    def decode( self, memory, src_mask, tgt, tgt_mask ):\n",
    "        return self.decoder( self.tgt_embed( tgt ), memory, src_mask, tgt_mask )\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator( nn.Module ):\n",
    "    '''Define standard linear + softmax generation step.'''\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Generator, self ).__init__()\n",
    "        self.proj = nn.Linear( d_model, vocab )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return F.log_softmax( self.proj( x ), dim=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones( module, N ):\n",
    "    '''Produce N identical layers.'''\n",
    "    return nn.ModuleList( [ copy.deepcopy( module ) for _ in range( N ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder( nn.Module ):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Encoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, mask )\n",
    "        return self.norm( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n",
    "\n",
    "EncoderLayer: 每层都有两个子层组成。第一个子层实现了“多头”的 Self-attention，第二个子层则是一个简单的Position-wise的全连接前馈网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, feed_forward, dropout ):\n",
    "        super( EncoderLayer, self ).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout=dropout ), 2 )\n",
    "        self.size = size\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, mask ) )\n",
    "        return self.sublayer[1]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__( self, features, eps=1e-6 ):\n",
    "        super( LayerNorm, self ).__init__()\n",
    "        self.a_2 = nn.Parameter( torch.ones( features ) )\n",
    "        self.b_2 = nn.Parameter( torch.zeros( features ) )\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward( self, x ):\n",
    "        mean = x.mean( -1, keepdim=True )\n",
    "        std = x.std( -1, keepdim=True )\n",
    "        return self.a_2 * ( x - mean ) / ( std * self.eps ) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection( nn.Module ):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    '''\n",
    "    def __init__( self, size, dropout ):\n",
    "        super( SublayerConnection, self ).__init__()\n",
    "        self.norm = LayerNorm( size )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x, sublayer ):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout( sublayer( self.norm( x ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "Decoder也是由N=6个相同层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder( nn.Module ):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, memory, src_mask, tgt_mask )\n",
    "        return self.norm( x ) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(DecoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer( nn.Module ):\n",
    "    \"Decoder is made of self_attn, src_attn, and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, src_attn, feed_forward, dropout ):\n",
    "        super( DecoderLayer, self ).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout ), 3 )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, tgt_mask ) )\n",
    "        x = self.sublayer[1]( x, lambda x: self.src_attn( x, x, x, src_mask ) )\n",
    "        return self.sublayer[2]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们还修改了解码器中的Self-attetion子层以防止当前位置attend到后续位置。这种Masked的Attention是考虑到输出Embedding会偏移一个位置，确保了生成位置i的预测时，仅依赖于i的位置处已知输出，相当于把后面不该看的信心屏蔽掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask( size ):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = ( 1, size, size )\n",
    "    subsequent_mask = np.triu( np.ones( attn_shape ), k=1 ).astype( 'uint8' )\n",
    "    return torch.from_numpy( subsequent_mask ) == 0\n",
    "\n",
    "plt.figure( figsize=( 5, 5 ) )\n",
    "plt.imshow( subsequent_mask( 19 )[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-c5dcddf20d8b2d7ce0130fac2071317d_720w.jpg\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention( query, key, value, mask=None, dropout=None ):\n",
    "    \"Compute 'Scaled Dot Product Attention\"\n",
    "    d_k = query.size( -1 )\n",
    "    scores = torch.matmul( query, key.transpose( -2, -1 )) \\\n",
    "             / math.sqrt( d_k )\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill( mask == 0.0, -1e9 )\n",
    "    p_attn = F.softmax( scores, dim=-1 )\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout( p_attn )\n",
    "    return torch.matmul( p_attn, value ), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHead:<br>\n",
    "\n",
    "<img  src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\" width=25% height=25% />\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, WKi∈Rdmodel×dk, WVi∈Rdmodel×dv and WO∈Rhdv×dmodel. In this work we employ h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "\n",
    "“多头”机制能让模型考虑到不同位置的Attention，另外“多头”Attention可以在不同的子空间表示不一样的关联关系，使用单个Head的Attention一般达不到这种效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeaderAttention( nn.Module ):\n",
    "    def __init__( self, h, d_model, dropout=0.1 ):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super( MultiHeaderAttention, self ).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones( nn.Linear( d_model, d_model ), 4 )\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, query, key, value, mask=None ):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze( 1 )\n",
    "        nbatches = query.size( 0 )\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [ l(x).view( nbatches, -1, self.h, self.d_k ).transpose( 1, 2 )\n",
    "              for l, x in zip( self.linears, ( query, key, value )) ]\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout )\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose( 1, 2 ).contingous() \\\n",
    "            .view( nbatches, -1, self.h * self.d_k )\n",
    "        return self.linears[-1]( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward( nn.Module ):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__( self, d_model, d_ff, dropout=0.1 ):\n",
    "        super( PositionwiseFeedForward, self ).__init__()\n",
    "        self.w_1 = nn.Linear( d_model, d_ff )\n",
    "        self.w_2 = nn.Linear( d_ff, d_model )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return seld.w_2( self.dropout( F.relu( self.w_1( x ) )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emedding 和 softmax\n",
    "\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-ca9c861576e2aac1ee7211d4e0bc6281_720w.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings( nn.Module ):\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Embeddings, self ).__init__()\n",
    "        self.lut = nn.Embedding( vocab, d_model )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return self.lut( x ) * math.sqrt( self.d_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding( nn.Module ):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__( self, d_model, dropout, max_len=5000 ):\n",
    "        super( PositionalEncoding, self ).__init__()\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros( max_len, d_model )\n",
    "        position = torch.arange( 0, max_len ).unsqueeze( 1 ).float()\n",
    "        div_term = torch.exp( torch.arange( 0., d_model, 2) * -(math.log(10000.0) / d_model ) )\n",
    "        pe[ :, 0::2 ] = torch.sin( position * div_term )\n",
    "        pe[ :, 1::2 ] = torch.cos( position * div_term )\n",
    "        pe = pe.unsqueeze( 0 )\n",
    "        self.register_buffer( 'pe', pe )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        x = x + Variable( self.pe[ :, :x.size(1) ], requires_grad=False )\n",
    "        return self.dropout( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=( 15, 5) )\n",
    "pe = PositionalEncoding( 20, 0 )\n",
    "y = pe.forward( Variable( torch.zeros( 1, 100, 20 ) ))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Here we define a function that takes in hyperparameters and produces a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeaderAttention( h, d_model )\n",
    "    ff = PositionwiseFeedForward( d_model, d_ff, dropout )\n",
    "    position = PositionalEncoding( d_model, dropout )\n",
    "\n",
    "    model = EncoderDecoder( \n",
    "        Encoder( EncoderLayer( d_model, c(attn), c(ff), dropout ), N ),\n",
    "        Decoder( DecoderLayer( d_model, c(attn), c(attn), c(ff), dropout ), N ),\n",
    "        nn.Sequential( Embeddings( d_model, src_vocab ), c(position) ),\n",
    "        nn.Sequential( Embeddings( d_model, tgt_vocab ), c(position) ),\n",
    "        Generator( d_model, tgt_vocab )\n",
    "    )\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform( p )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "print( tmp_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch( ):\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__( self, src, trg=None, pad=0 ):\n",
    "        self.src = src\n",
    "        self.src_mask = ( src != pad ).unsqueeze( -2 )\n",
    "        if trg is not None:\n",
    "            self.trg = trg[ :, :-1 ]\n",
    "            self.trg_y = trg[ :, 1: ]\n",
    "            self.trg_mask = self.make_std_mask( self.trg, pad  )\n",
    "            self.ntokens = ( self.trg_y != pad ).data.sum()\n",
    "\n",
    "     @staticmethod       \n",
    "    def make_std_mask( tgt, pad ):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = ( tgt != pad ).unsqueeze( -2 )\n",
    "        tgt_mask = tgt_mask & Variable( \n",
    "            subsequent_mask( tgt.size( -1 ).type_as( tgt_mask.data ))\n",
    "        )\n",
    "        return tgt_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Training LOoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch( data_iter, model, loss_compute ):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "\n",
    "    for i, batch in enumerate( data_iter ):\n",
    "        out = model.forward( batch.src, batch.trg, \\\n",
    "                             batch.src_mask, batch.trg_mask )\n",
    "        loss = loss_compute( out, batch.trg_y, batch.ntokens )\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print( 'Epoch Step: %4d Loss:%5.4f Tokens per sec  %4.2f' % ( i, loss / batch.ntokens, tokens / elapsed ))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    \n",
    "    return total_token / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn( new, count, sofar ):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max( max_src_in_batch, len( new.src ) )\n",
    "    max_tgt_in_batch = max( max_tgt_in_batch, len( new.trg ) + 2 )\n",
    "    src_elemnets = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "\n",
    "    return max( src_elemnets, tgt_elements )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "We used the Adam optimizer (cite) with β1=0.9, β2=0.98 and ϵ=10−9.\n",
    "\n",
    ">> Note: This part is very important. Need to train with this setup of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt():\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__( self, model_size, factor, warmup, optimizer ):\n",
    "        self.optimizer = optimizer\n",
    "        self.step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step( self ):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[ 'lr' ] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate( self, step=None ):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "                ( self.model_size ** ( -0.5 ) * \n",
    "                   min( step ** ( -0.5 ), step * self.warmup ** ( -1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt( model ):\n",
    "    return NoamOpt( model.src_embed[0].d_model, 2, 4000,\n",
    "                  torch.optim.Adam( model.parameters(), lr=0, betas=( 0.9, 0.98 ), eps=1e-9 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [ NoamOpt( 512, 1, 4000, None ),\n",
    "         NoamOpt( 1024, 1, 8000, None ),\n",
    "         NoamOpt( 256, 1, 4000, None )]\n",
    "plt.plot( np.arange( 1, 20000 ), [ [ opt.rate( i ) for opt in opts ] for i in range( 1, 20000 ) ])\n",
    "plt.legend( [ '512:4000', '1024:8000', '256:4000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Label Smoothing\n",
    "  mmnnrnrenrennren令人迷惑，但确实能改善accuracy and BLEU 成绩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing( nn.Module ):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__( self, size, padding_idx, smoothing=0.0 ):\n",
    "        super( LabelSmoothing, self ).__init__()\n",
    "        self.criterion = nn.KLDivLoss( size_average=False )\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward( self, x, target ):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_( self.smoothing / ( self.size - 2 ) )\n",
    "        true_dist.scatter_( 1, target.data.unsqueeze(1), self.confidence )\n",
    "        true_dist[ :, self.padding_idx ] = 0\n",
    "        mask = torch.nonzero( target.data == self.padding_idx )\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill( 0, mask.squeeze(), 0.0 )\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion( x, Variable( true_dist, requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'markdown' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2cd18ed6a16d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'markdown' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x15d3c240>"
     },
     "metadata": {},
     "execution_count": 29
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"245.400547pt\" version=\"1.1\" viewBox=\"0 0 396.867656 245.400547\" width=\"396.867656pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 245.400547 \r\nL 396.867656 245.400547 \r\nL 396.867656 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 51.367656 213.018984 \r\nL 386.167656 213.018984 \r\nL 386.167656 12.138984 \r\nL 51.367656 12.138984 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p9bb8ffc176)\">\r\n    <image height=\"201\" id=\"image56749abd15\" transform=\"scale(1 -1)translate(0 -201)\" width=\"335\" x=\"51.367656\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAU8AAADJCAYAAACjU7CDAAAABHNCSVQICAgIfAhkiAAAAuhJREFUeJzt1rEJQkEURcFVbMkCBAMLsIIPvyrBVuzLRBswOsmuMFPBDR6Hd7ge7p/BGGOM9+08ewLwJ46zBwD8I/EECMQTIBBPgEA8AQLxBAjEEyAQT4BAPAEC8QQIxBMgEE+AQDwBAvEECMQTIBBPgEA8AQLxBAjEEyAQT4BAPAEC8QQIxBMgEE+AQDwBAvEECMQTIBBPgEA8AQLxBAjEEyAQT4BAPAEC8QQIxBMgEE+AQDwBAvEECMQTIBBPgEA8AQLxBAjEEyAQT4BAPAEC8QQIxBMgEE+AQDwBAvEECMQTIBBPgEA8AQLxBAjEEyAQT4BAPAEC8QQIxBMgEE+AQDwBAvEECMQTIBBPgEA8AQLxBAhOswes5PV8zJ6wjMu2z54AS/N5AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQnGYPWMll22dPWMbr+Zg9YRnugl98ngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBF94sQutH2pnhwAAAABJRU5ErkJggg==\" y=\"-12.018984\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m5f4bf67158\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"84.847656\" xlink:href=\"#m5f4bf67158\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(80.712031 235.496953)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"151.807656\" xlink:href=\"#m5f4bf67158\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(147.672031 235.496953)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"218.767656\" xlink:href=\"#m5f4bf67158\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(214.632031 235.496953)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"285.727656\" xlink:href=\"#m5f4bf67158\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(281.592031 235.496953)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"352.687656\" xlink:href=\"#m5f4bf67158\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(348.552031 235.496953)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m12065879f1\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"12.138984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- −0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 17.077969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"45.618984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(18.093594 50.557969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"79.098984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(18.093594 84.037969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"112.578984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(18.093594 117.517969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"146.058984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(18.093594 150.997969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"179.538984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(18.093594 184.477969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"51.367656\" xlink:href=\"#m12065879f1\" y=\"213.018984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(18.093594 217.957969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 51.367656 213.018984 \r\nL 51.367656 12.138984 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 386.167656 213.018984 \r\nL 386.167656 12.138984 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 51.367656 213.018984 \r\nL 386.167656 213.018984 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 51.367656 12.138984 \r\nL 386.167656 12.138984 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9bb8ffc176\">\r\n   <rect height=\"200.88\" width=\"334.8\" x=\"51.367656\" y=\"12.138984\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD1CAYAAABDY8L3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing( 5, 0, 0.4 )\n",
    "predict = torch.FloatTensor( [[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]] )\n",
    "v = crit( Variable( predict.log() ), Variable( torch.LongTensor( [ 2, 1, 0 ] ) ) )\n",
    "plt.imshow( crit.true_dist )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "当model对选择给出非常有信心时， sjissjsjisjiisj实际上会开始惩罚model（防止过拟？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x15d9fa58>]"
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.18545pt\" version=\"1.1\" viewBox=\"0 0 385.974062 262.18545\" width=\"385.974062pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.18545 \r\nL 385.974062 262.18545 \r\nL 385.974062 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 40.474063 229.803887 \r\nL 375.274063 229.803887 \r\nL 375.274063 12.363887 \r\nL 40.474063 12.363887 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m4e8112988f\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"52.586493\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.450868 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"114.701521\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(106.430271 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"176.816549\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(168.545299 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"238.931576\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(230.660326 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"301.046604\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(292.775354 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"363.161632\" xlink:href=\"#m4e8112988f\" y=\"229.803887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(350.754757 252.281856)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc881b79e21\" style=\"stroke:#000000;stroke-width:1.3;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"219.920251\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 224.859235)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"178.363998\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 183.302982)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"136.807744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 141.746729)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"95.251491\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 100.190475)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"53.695238\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 58.634222)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:1.3;\" x=\"40.474063\" xlink:href=\"#mc881b79e21\" y=\"12.138984\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 17.077969)scale(0.13 -0.13)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pd69cae9f1c)\" d=\"M 55.692244 22.247524 \r\nL 58.797996 105.503193 \r\nL 61.903747 143.443413 \r\nL 65.009498 165.211245 \r\nL 68.11525 179.194458 \r\nL 71.221001 188.816067 \r\nL 74.326753 195.750769 \r\nL 77.432504 200.917901 \r\nL 80.538255 204.864362 \r\nL 83.644007 207.935736 \r\nL 86.749758 210.360785 \r\nL 89.85551 212.296762 \r\nL 92.961261 213.855103 \r\nL 96.067012 215.116866 \r\nL 99.172764 216.142295 \r\nL 102.278515 216.977053 \r\nL 105.384267 217.656264 \r\nL 108.490018 218.20738 \r\nL 111.595769 218.652132 \r\nL 114.701521 219.00789 \r\nL 117.807272 219.288719 \r\nL 120.913024 219.506059 \r\nL 124.018775 219.669349 \r\nL 127.124526 219.786401 \r\nL 130.230278 219.863728 \r\nL 133.336029 219.906807 \r\nL 136.44178 219.920251 \r\nL 139.547532 219.908005 \r\nL 142.653283 219.873411 \r\nL 145.759035 219.819337 \r\nL 148.864786 219.748242 \r\nL 151.970537 219.662284 \r\nL 155.076289 219.563306 \r\nL 158.18204 219.452906 \r\nL 161.287792 219.332506 \r\nL 164.393543 219.203329 \r\nL 167.499294 219.066457 \r\nL 170.605046 218.922842 \r\nL 173.710797 218.773311 \r\nL 176.816549 218.618633 \r\nL 179.9223 218.459434 \r\nL 183.028051 218.296313 \r\nL 186.133803 218.129789 \r\nL 189.239554 217.960311 \r\nL 192.345306 217.7883 \r\nL 195.451057 217.614128 \r\nL 198.556808 217.438107 \r\nL 201.66256 217.260548 \r\nL 204.768311 217.081709 \r\nL 207.874062 216.901814 \r\nL 210.979814 216.721087 \r\nL 214.085565 216.539726 \r\nL 217.191317 216.357903 \r\nL 220.297068 216.175746 \r\nL 223.402819 215.993411 \r\nL 226.508571 215.811019 \r\nL 229.614322 215.628691 \r\nL 232.720074 215.44653 \r\nL 235.825825 215.264607 \r\nL 238.931576 215.083024 \r\nL 242.037328 214.901837 \r\nL 245.143079 214.721124 \r\nL 248.248831 214.540942 \r\nL 251.354582 214.36134 \r\nL 254.460333 214.182387 \r\nL 257.566085 214.004093 \r\nL 260.671836 213.82651 \r\nL 263.777588 213.64968 \r\nL 266.883339 213.473615 \r\nL 269.98909 213.298358 \r\nL 273.094842 213.123939 \r\nL 276.200593 212.95035 \r\nL 279.306345 212.777636 \r\nL 282.412096 212.605807 \r\nL 285.517847 212.434859 \r\nL 288.623599 212.264845 \r\nL 291.72935 212.095715 \r\nL 294.835101 211.927534 \r\nL 297.940853 211.760268 \r\nL 301.046604 211.593962 \r\nL 304.152356 211.428583 \r\nL 307.258107 211.264143 \r\nL 310.363858 211.100658 \r\nL 313.46961 210.938118 \r\nL 316.575361 210.776528 \r\nL 319.681113 210.615892 \r\nL 322.786864 210.456194 \r\nL 325.892615 210.297452 \r\nL 328.998367 210.139649 \r\nL 332.104118 209.982782 \r\nL 335.20987 209.826857 \r\nL 338.315621 209.671863 \r\nL 341.421372 209.517799 \r\nL 344.527124 209.364664 \r\nL 347.632875 209.212439 \r\nL 350.738627 209.061137 \r\nL 353.844378 208.910739 \r\nL 356.950129 208.761246 \r\nL 360.055881 208.612638 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:2.275;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 40.474063 229.803887 \r\nL 40.474063 12.363887 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 375.274063 229.803887 \r\nL 375.274063 12.363887 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 40.474063 229.803887 \r\nL 375.274063 229.803887 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 40.474063 12.363887 \r\nL 375.274063 12.363887 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd69cae9f1c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.474063\" y=\"12.363887\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "crit = LabelSmoothing( 5, 0, 0.1 )\n",
    "def loss( x ):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor( [[ 0, x/d, 1/d, 1/d, 1/d ]] )\n",
    "    return crit( Variable( predict.log() ), Variable( torch.LongTensor( [1] ) ) ).item()\n",
    "\n",
    "plt.plot( np.arange(1, 100), [loss(x) for x in range(1, 100)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}