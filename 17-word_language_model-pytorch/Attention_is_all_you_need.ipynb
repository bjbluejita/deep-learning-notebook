{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention is All You Need\n",
    "源自：\n",
    "> http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/48731949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "from torch.autograd import  Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context( context='talk' )\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network.<br>\n",
    "![Transformer] (http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png \"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder( nn.Module ):\n",
    "    '''\n",
    "    A standard Encoder-Decoder architecture. Base for this          and many other models.\n",
    "    '''\n",
    "    def __init__( self, encoder, decoder, src_embed, tgt_embed, generator ):\n",
    "        super( EncoderDecoder, self ).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward( self, src, tgt, src_mask, tgt_mask ):\n",
    "        '''Take in and process masked src and target sequences.'''\n",
    "        return self.decode( self.encode( src, src_mask ), src_mask, tgt, tgt_mask )\n",
    "\n",
    "    def encode( self, src, src_mask ):\n",
    "        return self.encoder( self.src_embed( src ), src_mask )\n",
    "    \n",
    "    def decode( self, memory, src_mask, tgt, tgt_mask ):\n",
    "        return self.decoder( self.tgt_embed( tgt ), memory, src_mask, tgt_mask )\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator( nn.Module ):\n",
    "    '''Define standard linear + softmax generation step.'''\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Generator, self ).__init__()\n",
    "        self.proj = nn.Linear( d_model, vocab )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return F.log_softmax( self.proj( x ), dim=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "The encoder is composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones( module, N ):\n",
    "    '''Produce N identical layers.'''\n",
    "    return nn.ModuleList( [ copy.deepcopy( module ) for _ in range( N ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder( nn.Module ):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Encoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, mask )\n",
    "        return self.norm( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n",
    "\n",
    "EncoderLayer: 每层都有两个子层组成。第一个子层实现了“多头”的 Self-attention，第二个子层则是一个简单的Position-wise的全连接前馈网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer( nn.Module ):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, feed_forward, dropout ):\n",
    "        super( EncoderLayer, self ).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout=dropout ), 2 )\n",
    "        self.size = size\n",
    "\n",
    "    def forward( self, x, mask ):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, mask ) )\n",
    "        return self.sublayer[1]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__( self, features, eps=1e-6 ):\n",
    "        super( LayerNorm, self ).__init__()\n",
    "        self.a_2 = nn.Parameter( torch.ones( features ) )\n",
    "        self.b_2 = nn.Parameter( torch.zeros( features ) )\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward( self, x ):\n",
    "        mean = x.mean( -1, keepdim=True )\n",
    "        std = x.std( -1, keepdim=True )\n",
    "        return self.a_2 * ( x - mean ) / ( std * self.eps ) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection( nn.Module ):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    '''\n",
    "    def __init__( self, size, dropout ):\n",
    "        super( SublayerConnection, self ).__init__()\n",
    "        self.norm = LayerNorm( size )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x, sublayer ):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout( sublayer( self.norm( x ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "Decoder也是由N=6个相同层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder( nn.Module ):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__( self, layer, N ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.layers = clones( layer, N )\n",
    "        self.norm = LayerNorm( layer.size )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        for layer in self.layers:\n",
    "            x = layer( x, memory, src_mask, tgt_mask )\n",
    "        return self.norm( x ) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(DecoderLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer( nn.Module ):\n",
    "    \"Decoder is made of self_attn, src_attn, and feed forward (defined below)\"\n",
    "    def __init__( self, size, self_attn, src_attn, feed_forward, dropout ):\n",
    "        super( DecoderLayer, self ).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection( size, dropout ), 3 )\n",
    "\n",
    "    def forward( self, x, memory, src_mask, tgt_mask ):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0]( x, lambda x: self.self_attn( x, x, x, tgt_mask ) )\n",
    "        x = self.sublayer[1]( x, lambda x: self.src_attn( x, m, m, src_mask ) )\n",
    "        return self.sublayer[2]( x, self.feed_forward )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们还修改了解码器中的Self-attetion子层以防止当前位置attend到后续位置。这种Masked的Attention是考虑到输出Embedding会偏移一个位置，确保了生成位置i的预测时，仅依赖于i的位置处已知输出，相当于把后面不该看的信心屏蔽掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask( size ):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = ( 1, size, size )\n",
    "    subsequent_mask = np.triu( np.ones( attn_shape ), k=1 ).astype( 'uint8' )\n",
    "    return torch.from_numpy( subsequent_mask ) == 0\n",
    "\n",
    "plt.figure( figsize=( 5, 5 ) )\n",
    "plt.imshow( subsequent_mask( 19 )[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-c5dcddf20d8b2d7ce0130fac2071317d_720w.jpg\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention( query, key, value, mask=None, dropout=None ):\n",
    "    \"Compute 'Scaled Dot Product Attention\"\n",
    "    d_k = query.size( -1 )\n",
    "    scores = torch.matmul( query, key.transpose( -2, -1 )) \\\n",
    "             / math.sqrt( d_k )\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill( mask == 0.0, -1e9 )\n",
    "    p_attn = F.softmax( scores, dim=-1 )\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout( p_attn )\n",
    "    return torch.matmul( p_attn, value ), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHead:<br>\n",
    "\n",
    "<img  src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png\" width=25% height=25% />\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, WKi∈Rdmodel×dk, WVi∈Rdmodel×dv and WO∈Rhdv×dmodel. In this work we employ h=8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "\n",
    "“多头”机制能让模型考虑到不同位置的Attention，另外“多头”Attention可以在不同的子空间表示不一样的关联关系，使用单个Head的Attention一般达不到这种效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeaderAttention( nn.Module ):\n",
    "    def __init__( self, h, d_model, dropout=0.1 ):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super( MultiHeaderAttention, self ).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones( nn.Linear( d_model, d_model ), 4 )\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, query, key, value, mask=None ):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze( 1 )\n",
    "        nbatches = query.size( 0 )\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [ l(x).view( nbatches, -1, self.h, self.d_k ).transpose( 1, 2 )\n",
    "              for l, x in zip( self.linears, ( query, key, value )) ]\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout )\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose( 1, 2 ).contiguous() \\\n",
    "            .view( nbatches, -1, self.h * self.d_k )\n",
    "        return self.linears[-1]( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward( nn.Module ):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__( self, d_model, d_ff, dropout=0.1 ):\n",
    "        super( PositionwiseFeedForward, self ).__init__()\n",
    "        self.w_1 = nn.Linear( d_model, d_ff )\n",
    "        self.w_2 = nn.Linear( d_ff, d_model )\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return self.w_2( self.dropout( F.relu( self.w_1( x ) )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emedding 和 softmax\n",
    "\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-ca9c861576e2aac1ee7211d4e0bc6281_720w.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings( nn.Module ):\n",
    "    def __init__( self, d_model, vocab ):\n",
    "        super( Embeddings, self ).__init__()\n",
    "        self.lut = nn.Embedding( vocab, d_model )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return self.lut( x ) * math.sqrt( self.d_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding( nn.Module ):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__( self, d_model, dropout, max_len=5000 ):\n",
    "        super( PositionalEncoding, self ).__init__()\n",
    "        self.dropout = nn.Dropout( p=dropout )\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros( max_len, d_model )\n",
    "        position = torch.arange( 0, max_len ).unsqueeze( 1 ).float()\n",
    "        div_term = torch.exp( torch.arange( 0., d_model, 2) * -(math.log(10000.0) / d_model ) )\n",
    "        pe[ :, 0::2 ] = torch.sin( position * div_term )\n",
    "        pe[ :, 1::2 ] = torch.cos( position * div_term )\n",
    "        pe = pe.unsqueeze( 0 )\n",
    "        self.register_buffer( 'pe', pe )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        x = x + Variable( self.pe[ :, :x.size(1) ], requires_grad=False )\n",
    "        return self.dropout( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=( 15, 5) )\n",
    "pe = PositionalEncoding( 20, 0 )\n",
    "y = pe.forward( Variable( torch.zeros( 1, 100, 20 ) ))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Here we define a function that takes in hyperparameters and produces a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeaderAttention( h, d_model )\n",
    "    ff = PositionwiseFeedForward( d_model, d_ff, dropout )\n",
    "    position = PositionalEncoding( d_model, dropout )\n",
    "\n",
    "    model = EncoderDecoder( \n",
    "        Encoder( EncoderLayer( d_model, c(attn), c(ff), dropout ), N ),\n",
    "        Decoder( DecoderLayer( d_model, c(attn), c(attn), c(ff), dropout ), N ),\n",
    "        nn.Sequential( Embeddings( d_model, src_vocab ), c(position) ),\n",
    "        nn.Sequential( Embeddings( d_model, tgt_vocab ), c(position) ),\n",
    "        Generator( d_model, tgt_vocab )\n",
    "    )\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform( p )\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "print( tmp_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch( ):\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__( self, src, trg=None, pad=0 ):\n",
    "        self.src = src\n",
    "        self.src_mask = ( src != pad ).unsqueeze( -2 )\n",
    "        if trg is not None:\n",
    "            self.trg = trg[ :, :-1 ]\n",
    "            self.trg_y = trg[ :, 1: ]\n",
    "            self.trg_mask = self.make_std_mask( self.trg, pad  )\n",
    "            self.ntokens = ( self.trg_y != pad ).data.sum()\n",
    "\n",
    "    @staticmethod       \n",
    "    def make_std_mask( tgt, pad ):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = ( tgt != pad ).unsqueeze( -2 )\n",
    "        tgt_mask = tgt_mask & Variable( \n",
    "            subsequent_mask( tgt.size( -1 ) ).type_as( tgt_mask.data )\n",
    "        )\n",
    "        return tgt_mask\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch( data_iter, model, loss_compute ):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "\n",
    "    for i, batch in enumerate( data_iter ):\n",
    "        out = model.forward( batch.src, batch.trg, \\\n",
    "                             batch.src_mask, batch.trg_mask )\n",
    "        loss = loss_compute( out, batch.trg_y, batch.ntokens )\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print( 'Epoch Step: %4d Loss:%5.4f Tokens per sec  %4.2f' % ( i, loss / batch.ntokens, tokens / elapsed ))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn( new, count, sofar ):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max( max_src_in_batch, len( new.src ) )\n",
    "    max_tgt_in_batch = max( max_tgt_in_batch, len( new.trg ) + 2 )\n",
    "    src_elemnets = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "\n",
    "    return max( src_elemnets, tgt_elements )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "We used the Adam optimizer (cite) with β1=0.9, β2=0.98 and ϵ=10−9.\n",
    "\n",
    ">> Note: This part is very important. Need to train with this setup of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt():\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__( self, model_size, factor, warmup, optimizer ):\n",
    "        self.optimizer = optimizer\n",
    "        self.step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step( self ):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[ 'lr' ] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate( self, step=None ):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "                ( self.model_size ** ( -0.5 ) * \n",
    "                   min( step ** ( -0.5 ), step * self.warmup ** ( -1.5)))\n",
    "\n",
    "\n",
    "def get_std_opt( model ):\n",
    "    return NoamOpt( model.src_embed[0].d_model, 2, 4000,\n",
    "                  torch.optim.Adam( model.parameters(), lr=0, betas=( 0.9, 0.98 ), eps=1e-9 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [ NoamOpt( 512, 1, 4000, None ),\n",
    "         NoamOpt( 1024, 1, 8000, None ),\n",
    "         NoamOpt( 256, 1, 4000, None )]\n",
    "plt.plot( np.arange( 1, 20000 ), [ [ opt.rate( i ) for opt in opts ] for i in range( 1, 20000 ) ])\n",
    "plt.legend( [ '512:4000', '1024:8000', '256:4000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Label Smoothing\n",
    "  mmnnrnrenrennren令人迷惑，但确实能改善accuracy and BLEU 成绩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing( nn.Module ):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__( self, size, padding_idx, smoothing=0.0 ):\n",
    "        super( LabelSmoothing, self ).__init__()\n",
    "        self.criterion = nn.KLDivLoss( size_average=False )\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward( self, x, target ):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_( self.smoothing / ( self.size - 2 ) )\n",
    "        true_dist.scatter_( 1, target.data.unsqueeze(1), self.confidence )\n",
    "        true_dist[ :, self.padding_idx ] = 0\n",
    "        mask = torch.nonzero( target.data == self.padding_idx )\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill( 0, mask.squeeze(), 0.0 )\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion( x, Variable( true_dist, requires_grad=False ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of label smoothing.\n",
    "crit = LabelSmoothing( 5, 0, 0.4 )\n",
    "predict = torch.FloatTensor( [[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]] )\n",
    "v = crit( Variable( predict.log() ), Variable( torch.LongTensor( [ 2, 1, 0 ] ) ) )\n",
    "plt.imshow( crit.true_dist )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "当model对选择给出非常有信心时， sjissjsjisjiisj实际上会开始惩罚model（防止过拟？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = LabelSmoothing( 5, 0, 0.1 )\n",
    "def loss( x ):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor( [[ 0, x/d, 1/d, 1/d, 1/d ]] )\n",
    "    return crit( Variable( predict.log() ), Variable( torch.LongTensor( [1] ) ) ).item()\n",
    "\n",
    "plt.plot( np.arange(1, 100), [loss(x) for x in range(1, 100)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "def data_gen( V, batch, nbatches ):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range( nbatches ):\n",
    "        data = torch.from_numpy( np.random.randint( 1, V, size=( batch, 10 ) ) )\n",
    "        data[ :, 0 ] = 1\n",
    "        src = Variable( data, requires_grad=False )\n",
    "        tgt = Variable( data, requires_grad=False )\n",
    "        yield Batch( src, tgt, 0 )\n",
    "\n",
    "# Loss Computation\n",
    "class SimpleLossCompute():\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__( self, generator, criterion, opt=None ):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__( self, x, y, norm ):\n",
    "        x = self.generator( x )\n",
    "        loss = self.criterion( x.contiguous().view( -1, x.size(-1) ),\n",
    "                               y.contiguous().view( -1 ) ) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm\n",
    "\n",
    "# Greedy Decoding\n",
    "V = 11\n",
    "criterion = LabelSmoothing( size=V, padding_idx=0, smoothing=0.0 )\n",
    "model = make_model( V, V, N=2 )\n",
    "model_opt = NoamOpt( model.src_embed[0].d_model, 1, 400,\n",
    "                     torch.optim.Adam( model.parameters(), lr=1, betas=( 0.9, 0.98 ), eps=1e-9 ) )\n",
    "\n",
    "for epoch in range( 10 ):\n",
    "    model.train()\n",
    "    run_epoch( data_gen( V, 30, 20 ), model, \n",
    "                SimpleLossCompute( model.generator, criterion, model_opt ))\n",
    "    model.eval()\n",
    "    print( run_epoch(data_gen( V, 30, 5 ), model, \n",
    "                    SimpleLossCompute( model.generator, criterion, None ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets\n",
    "\n",
    "if True:\n",
    "    import spacy\n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "\n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "    BLANK_WORD = \"<blank>\"\n",
    "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "    MAX_LEN = 100\n",
    "    train, val, test = datasets.IWSLT.splits( exts=( '.de', '.en' ), fields=( SRC, TGT ),\n",
    "                                               filter_pred=lambda x: len( vars(x)[ 'src']) <= MAX_LEN and\n",
    "                                                  len( vars(x)[ 'trg' ]) <= MAX_LEN )\n",
    "    MIN_FREQ = 2\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator( data.Iterator ):\n",
    "    def create_batches( self ):\n",
    "        if self.train:\n",
    "            def pool( d, random_shuffler ):\n",
    "                for p in data.batch( d, self.batch_size * 100 ):\n",
    "                    p_batch = data.batch( sorted( p, key=self.sort_key ),\n",
    "                                           self.batch_size, self.batch_size_fn )\n",
    "                    for b in random_shuffler( list( p_batch ) ):\n",
    "                        yield b\n",
    "            self.batches = pool( self.data(), self.random_shuffler )\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch( self.data(), self.batch_size, self.batch_size_fn ):\n",
    "                self.batches.append( sorted( b, key=self.sort_key ) )\n",
    "\n",
    "def rebatch( pad_idx, batch ):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose( 0, 1 ), batch.trg.transpose( 0, 1 )\n",
    "    return Batch( src, trg, pad_idx )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "最后为了快速训练，我们使用了多块     GPU。这段代码将实现多 GPU 的词生成，但它并不是针对 Transformer 的具体方法，所以这里并不会具体讨论。多 GPU 训练的基本思想即在训练过程中将词生成分割为语块（chunks），并传入不同的 GPU 实现并行处理，我们可以使用 PyTorch 并行基元实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__( self, generator, criterion, devices, opt=None, chunk_size=5 ):\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate( criterion, devices=devices )\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__( self, out, targets, normalize ):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate( self.generator, devices=devices )\n",
    "        out_scatter = nn.parallel.scatter( out, target_gpus=self.devices )\n",
    "        out_grad = [ [] for _ in out_scatter ]\n",
    "        targets = nn.parallel.scatter( targets, target_gpus=self.devices )\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range( 0, out_scatter[0].size(1), chunk_size ):\n",
    "            # Predict distributions\n",
    "            out_column = [ [ Variable( o[ :, i:i+chunk_size ].data, requires_grad=self.opt is not None )]\n",
    "                           for o in out_scatter ]\n",
    "            gen = nn.parallel.parallel_apply( generator, out_column )\n",
    "\n",
    "            # Compute loss\n",
    "            y = [ ( g.contigous().view( -1, g.size( -1 )), \n",
    "                   t[ :, i:i+chunk_size ].contigous().view( -1 ))\n",
    "                     for g, t in zip( gen, targets ) ]\n",
    "            loss = nn.parallel.parallel_apply( self.criterion, y )\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather( loss, target_device=self.devices[0] )\n",
    "            l = l.sum()[0] / normalize\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate( loss ):\n",
    "                    out_grad[ j ].append( out_column[ j ][ 0 ].grad.data.clone() ) \n",
    "\n",
    "        # Backprop loss to output of transformer\n",
    "        if self.opt is not None:\n",
    "            out_grad = [ Variable( torch.cat( og, dim=1 )) for og in out_grad ]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather( out_grad,\n",
    "                                     target_device=self.devices[0] )\n",
    "            o1.backward( gradient=o2 )\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "\n",
    "        return total * normalize                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们利用前面定义的函数创建了模型、度量标准、优化器、数据迭代器和并行化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [0]\n",
    "if True:\n",
    "    pad_idx = TGT.vocab.stoi[ '<blank>' ]\n",
    "    model =make_model( len(SRC.vocab ), len(TGT.vocab ), N=6 )\n",
    "    model.cuda()\n",
    "    criterion = LabelSmoothing( size=len( TGT.vocab ), padding_idx=pad_idx, smoothing=0.1 )\n",
    "    criterion.cuda()\n",
    "    BATCH_SIZE = 12000\n",
    "    train_iter = MyIterator( train, batch_size=BATCH_SIZE, device=0, repeat=False, \n",
    "                             sort_key=lambda x: ( len( x.src ), len( x.trg ) ), \n",
    "                             batch_size_fn=batch_size_fn, train=True )\n",
    "    valid_iter = MyIterator( val, batch_size=BATCH_SIZE, device=0, repeat=False,\n",
    "                              sort_key=lambda x: ( len( x.src ), len( x.trg ) ),\n",
    "                              batch_size_fn=batch_size_fn, train=False )\n",
    "    model_par = nn.DataParallel( model, device_ids=devices )\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model_opt = NoamOpt( model.src_embed[0].d_model, 1, 2000,\n",
    "                         torch.optim.Adam( model.parameters(), lr=0, betas=( 0.9, 0.98 ), eps=1e-9 ) )\n",
    "\n",
    "    for epoch in range( 10 ):\n",
    "        model_par.train()\n",
    "        run_epoch( ( rebatch( pad_idx, b ) for b in train_iter ),\n",
    "                    model_par,\n",
    "                    MultiGPULossCompute( model.generator, criterion,\n",
    "                                         devices=devices, opt=model_opt ))\n",
    "        model_par.eval()\n",
    "        loss = run_epoch( ( rebatch( pad_idx, b ) for b in valid_iter ),\n",
    "                           model_par, \n",
    "                           MultiGPULossCompute( model.generator, criterion,\n",
    "                                                devices=devices, opt=None ) )\n",
    "        print( loss )\n",
    "else:\n",
    "    model = torch.load( 'iwslt.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}